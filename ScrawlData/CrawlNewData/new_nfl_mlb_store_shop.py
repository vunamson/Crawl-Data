from datetime import datetime
import random
from sys import _xoptions
from bs4 import BeautifulSoup
import undetected_chromedriver as uc
from selenium.common.exceptions import NoSuchElementException, TimeoutException
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import pandas as pd
import time

# 1Ô∏è‚É£ C·∫•u h√¨nh Selenium (tƒÉng t·ªëc tr√¨nh duy·ªát)
# chrome_options = Options()
# chrome_options.add_argument("--headless")  # Ch·∫°y kh√¥ng hi·ªÉn th·ªã tr√¨nh duy·ªát
# chrome_options.add_argument("--disable-gpu")  # T·∫Øt GPU tƒÉng hi·ªáu su·∫•t
# chrome_options.add_argument("--no-sandbox")  # B·ªè h·∫°n ch·∫ø sandbox
# chrome_options.add_argument("--disable-dev-shm-usage")  # H·ªó tr·ª£ ch·∫°y tr√™n server
# chrome_options.add_argument("--disable-blink-features=AutomationControlled")  # Gi·∫£m kh·∫£ nƒÉng b·ªã ph√°t hi·ªán bot
# chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36")

# üìå ƒê·ªçc danh s√°ch proxy t·ª´ file IpUs.txt
# def load_proxies(file_path="IpUs.txt"):
#     with open(file_path, "r") as file:
#         proxies = [line.strip() for line in file.readlines() if line.strip()]
#     return proxies

# üõ†Ô∏è Ki·ªÉm tra proxy c√≥ ho·∫°t ƒë·ªông kh√¥ng
# def is_proxy_working(proxy):
#     test_url = "https://api64.ipify.org?format=json"  # API ki·ªÉm tra IP
#     proxies = {
#         "http": f"http://{proxy}",
#         "https": f"http://{proxy}",
#     }
#     try:
#         response = requests.get(test_url, proxies=proxies, timeout=5)
#         if response.status_code == 200:
#             print(f"‚úÖ Proxy ho·∫°t ƒë·ªông: {proxy}")
#             return True
#     except requests.RequestException:
#         pass
#     print(f"‚ùå Proxy kh√¥ng ho·∫°t ƒë·ªông: {proxy}")
#     return False

# def get_random_proxy(proxies):
#     random.shuffle(proxies)  # X√°o tr·ªôn danh s√°ch proxy
#     for proxy in proxies:
#         if is_proxy_working(proxy):
#             return proxy
#     return None


def get_random_user_agent():    
    chrome_version = f"{random.randint(2, 200)}.0.0.0"  # T·∫°o s·ªë ng·∫´u nhi√™n t·ª´ 2.0.0.0 - 200.0.0.0
    return f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{chrome_version} Safari/537.36"

# proxies = load_proxies()

# üõ†Ô∏è H√†m t·∫°o WebDriver v·ªõi User-Agent m·ªõi
def create_driver(): 
    chrome_options = uc.ChromeOptions()
    # chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.set_capability('LT:Options', _xoptions)

    # if proxy:
    #     chrome_options.add_argument(f"--proxy-server={proxy}")
    #     print(f"üîÑ ƒêang s·ª≠ d·ª•ng Proxy: {proxy}")
    # üîÑ Th√™m User-Agent ng·∫´u nhi√™n
    user_agent = get_random_user_agent()
    chrome_options.add_argument(f"user-agent={user_agent}")
    
    print(f"üÜï ƒê√£ thay ƒë·ªïi User-Agent: {user_agent}")
    # return uc.Chrome(headless=False)
    return uc.Chrome(headless=False)
# Kh·ªüi t·∫°o WebDriver duy nh·∫•t (tr√°nh t·∫°o l·∫°i m·ªói l·∫ßn l·∫∑p)
# current_proxy = get_random_proxy(proxies)
def check_publish_date(driver, url):
    try:
        # driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(3)
        page_source = driver.page_source
        # source = driver.execute_cdp_cmd("Page.getResourceContent", {"frameId": "1", "url": url})
        webGoLiveDate = None
        # print('webGoLiveDate',webGoLiveDate)
        str_page_source = str(page_source)
        print('len' , len(str_page_source))
        indices = [i for i in range(len(str_page_source)) if str_page_source.startswith("webGoLiveDate", i)]
        for idx in indices:
            start_idx = idx + len("webGoLiveDate") +2  # ƒê·∫ßu ti√™n sau "webGoLiveDate"
            end_idx = start_idx + 10  # L·∫•y 10 k√Ω t·ª± sau ƒë√≥
            snippet = page_source[start_idx:end_idx]
            print('snippet' , snippet )
            if snippet.isdigit():                                                
                webGoLiveDate = int(snippet)
            else : 'khong phai so nguyen'
        print('webGoLiveDate' ,webGoLiveDate)
        if webGoLiveDate:
            # start_idx = webGoLiveDate + len(webGoLiveDate) + 2
            # end_idx = page_source.find(',', start_idx)
            # timestamp_str = page_source[start_idx:end_idx].strip()
            
            # Chuy·ªÉn Unix timestamp sang datetime
            publish_date = datetime.fromtimestamp(int(webGoLiveDate))
            print('publish_date',publish_date)
            start_date = datetime(2025, 3, 1, 0, 0, 0)
            end_date = datetime(2025, 4, 9, 0, 0, 0)
            print('start_date' ,start_date <= publish_date < end_date )
            if start_date <= publish_date < end_date :
                return publish_date
            else: return None
    except Exception as e:
        print(f"L·ªói ki·ªÉm tra ng√†y ƒëƒÉng: {e}")
    return None 

def check_publish_date_false(driver, url):
    try:
        # driver.get(url)
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(3)
        page_source = driver.page_source
        # source = driver.execute_cdp_cmd("Page.getResourceContent", {"frameId": "1", "url": url})
        webGoLiveDate = None
        # print('webGoLiveDate',webGoLiveDate)
        str_page_source = str(page_source)
        print('len' , len(str_page_source))
        indices = [i for i in range(len(str_page_source)) if str_page_source.startswith("webGoLiveDate", i)]
        for idx in indices:
            start_idx = idx + len("webGoLiveDate") +2  # ƒê·∫ßu ti√™n sau "webGoLiveDate"
            end_idx = start_idx + 10  # L·∫•y 10 k√Ω t·ª± sau ƒë√≥
            snippet = page_source[start_idx:end_idx]
            print('snippet' , snippet )
            if snippet.isdigit():                                                
                webGoLiveDate = int(snippet)
            else : 'khong phai so nguyen'
        print('webGoLiveDate' ,webGoLiveDate)
        if webGoLiveDate:
            # start_idx = webGoLiveDate + len(webGoLiveDate) + 2
            # end_idx = page_source.find(',', start_idx)
            # timestamp_str = page_source[start_idx:end_idx].strip()
            
            # Chuy·ªÉn Unix timestamp sang datetime
            publish_date = datetime.fromtimestamp(int(webGoLiveDate))
            start_date = datetime(2025, 3, 1, 0, 0, 0)
            end_date = datetime(2025, 4, 9, 0, 0, 0)
            print('start_date' ,start_date <= publish_date < end_date )
            return start_date > publish_date 
    except Exception as e:
        print(f"L·ªói ki·ªÉm tra ng√†y ƒëƒÉng: {e}")
    return False 


driver = create_driver()
product_links = []
url = "https://www.mlbshop.com/jerseys/d-1250336792+z-990485-1677480024?sortOption=NewestArrivals"
driver.get(url)
time.sleep(5)
count = 0

while True:
    try:
        if(count == 40) : break
        count = count + 1
        # ƒê·ª£i trang t·∫£i ho√†n t·∫•t tr∆∞·ªõc khi l·∫•y d·ªØ li·ªáu
        WebDriverWait(driver, 5).until(
            EC.presence_of_element_located((By.TAG_NAME, 'body'))
        )
        # L·∫•y t·∫•t c·∫£ ·∫£nh s·∫£n ph·∫©m tr√™n trang hi·ªán t·∫°i
        links = driver.find_elements(By.CSS_SELECTOR, "a.color-black")
        print('links', len(links)) 
        # L∆∞u danh s√°ch c√°c link s·∫£n ph·∫©m
        for link in links:
            href = link.get_attribute("href")
            if href and href not in product_links:
                # if check_publish_date(browser, href):
                product_links.append(href)

        # Ki·ªÉm tra xem c√≥ n√∫t next page hay kh√¥ng
        try:
            next_button = WebDriverWait(driver, 5).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "a[data-trk-id='next-page']"))
            )
            aria_disabled = next_button.get_attribute("aria-disabled")

            if aria_disabled == "true":
                break  # Kh√¥ng c√≤n trang ti·∫øp theo

            next_button.click()

            # ƒê·ª£i trang m·ªõi t·∫£i ho√†n t·∫•t
            # WebDriverWait(browser, 10).until(
            #     EC.staleness_of(images[0])  # Ch·ªù cho trang c≈© bi·∫øn m·∫•t
            # )
        except (NoSuchElementException, TimeoutException):
            break

    except TimeoutException:
        print("Kh√¥ng t√¨m th·∫•y ph·∫ßn t·ª≠ mong mu·ªën, tho√°t ch∆∞∆°ng tr√¨nh.")
        break


# 3Ô∏è‚É£ Crawl d·ªØ li·ªáu t·ª´ng s·∫£n ph·∫©m
data = []
for index, link in enumerate(product_links):
    try:
        # print('index' , index)
        # if index > 0 and index  == 1 : 
            # driver.quit()
            # driver = create_driver()
            # driver.delete_all_cookies()
            # print('if')
            # current_proxy = get_random_proxy(proxies)
            # driver = create_driver(current_proxy)
        # else : print('else')
        driver.get(link)
        # driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
        if check_publish_date_false(driver,link) : break
        date = check_publish_date(driver,link)
        if date :

        # if index == 0 : check_publish_date(driver,link)
        # else: break
            print(f"üîç ƒêang l·∫•y d·ªØ li·ªáu t·ª´: {link} ({index + 1}/{len(product_links)})")
            # S·ª≠ d·ª•ng WebDriverWait thay v√¨ time.sleep()
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, 'h1[data-talos="labelPdpProductTitle"]'))
            )

            # L·∫•y t√™n s·∫£n ph·∫©m

            # T√¨m danh s√°ch m√†u s·∫Øc (n·∫øu c√≥)
            color_buttons = driver.find_elements(By.CSS_SELECTOR, 'a[data-trk-id="color-selector"]')

            if color_buttons:
                for button in color_buttons:
                    
                    try:
                        driver.execute_script("arguments[0].click();", button)
                        time.sleep(1)  # Gi·∫£m th·ªùi gian sleep
                        name_element = driver.find_element(By.CSS_SELECTOR, 'h1[data-talos="labelPdpProductTitle"]')
                        name = name_element.text.strip() if name_element else "N/A"
                        # L·∫•y danh s√°ch ·∫£nh sau khi ch·ªçn m√†u
                        img_elements = driver.find_elements(By.CSS_SELECTOR, 'img.thumbnail-image')
                        img_links = [img.get_attribute("src") for img in img_elements]

                        # L∆∞u d·ªØ li·ªáu
                        img_links_str = ", ".join(img_links) if img_links else "L·ªñI"
                        data.append([name, img_links_str,date])
                        print(f"‚úÖ {name} ({len(img_links)} ·∫£nh)")
                    except Exception as e:
                        print(f"‚ö†Ô∏è L·ªói khi click m√†u: {e}")
                        data.append([name, "L·ªñI"])
            else:
                name_element = driver.find_element(By.CSS_SELECTOR, 'h1[data-talos="labelPdpProductTitle"]')
                name = name_element.text.strip() if name_element else "N/A"
                img_elements = driver.find_elements(By.CSS_SELECTOR, 'img.thumbnail-image')
                img_links = [img.get_attribute("src") for img in img_elements]

                img_links_str = ", ".join(img_links) if img_links else "L·ªñI"
                data.append([name, img_links_str,date])
                print(f"‚úÖ {name} ({len(img_links)} ·∫£nh)")

    except Exception as e:
        print(f"‚ùå L·ªói khi crawl {link}: {e}")
        driver.quit()
        time.sleep(10)
        driver = create_driver()
        data.append(["L·ªñI", "L·ªñI"])

# 4Ô∏è‚É£ L∆∞u d·ªØ li·ªáu v√†o file Excel
driver.quit()
del driver
df = pd.DataFrame(data, columns=["Name", "Image Links","Date"])
df.to_excel("output_mlb_shop_1-3--9-4.xlsx", index=False, engine="openpyxl")

print("üéâ D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u v√†o output1.xlsx!")