from datetime import datetime
import random
import time
import gspread
from oauth2client.service_account import ServiceAccountCredentials
import pandas as pd
from selenium.common.exceptions import NoSuchElementException, TimeoutException
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC


# ğŸ”¹ HÃ m táº¡o User-Agent ngáº«u nhiÃªn
def get_random_user_agent():
    chrome_version = f"{random.randint(2, 200)}.0.0.0"
    return f"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{chrome_version} Safari/537.36"


# ğŸ”¹ HÃ m khá»Ÿi táº¡o trÃ¬nh duyá»‡t Selenium vá»›i User-Agent ngáº«u nhiÃªn
def create_driver():
    chrome_options = uc.ChromeOptions()
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    user_agent = get_random_user_agent()
    chrome_options.add_argument(f"user-agent={user_agent}")

    print(f"ğŸ†• ÄÃ£ thay Ä‘á»•i User-Agent: {user_agent}")
    return uc.Chrome(headless=False)


# ğŸ”¹ HÃ m láº¥y danh sÃ¡ch URL tá»« Google Sheets (Sheet "Torunstyle")
def get_urls_from_sheets():
    try:
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_name(r"C:\ScrawlData\CrawlNewData\credentials.json", scope)
        client = gspread.authorize(creds)
        sheet_id = "1c_omcHE3AMkUBxFS1Cckwcw0A-fBX5FV6dbJYn_Jg5s"
        sheet = client.open_by_key(sheet_id).worksheet("Torunstyle")

        urls = sheet.col_values(1)[1:]  # Láº¥y cá»™t A, bá» qua tiÃªu Ä‘á»
        extra_data = sheet.col_values(2)[1:]  # Láº¥y cá»™t B (náº¿u cÃ³)

        url_data  = []
        for i, url in enumerate(urls):
            url_data.append({
                "url" : url,
                "types" : extra_data[i]
            })

        print(f"âœ… ÄÃ£ láº¥y {len(urls)} URL tá»« Google Sheets.")
        return url_data
    except Exception as e:
        print(f"âŒ Lá»—i khi láº¥y dá»¯ liá»‡u tá»« Google Sheets: {e}")
        return {}


# ğŸ”¹ HÃ m kiá»ƒm tra ngÃ y Ä‘Äƒng sáº£n pháº©m
def check_publish_date(driver, url):
    try:
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, "body")))
        time.sleep(3)
        page_source = driver.page_source
        indices = [i for i in range(len(page_source)) if page_source.startswith("datePublished", i)]

        for idx in indices:
            start_idx = idx + len("datePublished") + 3
            end_idx = start_idx + 25
            snippet = page_source[start_idx:end_idx]
            try:
                date_published = datetime.strptime(snippet, "%Y-%m-%dT%H:%M:%S%z")
                publish_date = date_published.replace(tzinfo=None)
                print(f"ğŸ“… NgÃ y Ä‘Äƒng: {publish_date}")
                start_date = datetime(2025, 3, 3, 0, 0, 0)
                end_date = datetime(2025, 3, 6, 0, 0, 0)
                if start_date <= publish_date < end_date:
                    return publish_date
            except ValueError:
                continue
    except Exception as e:
        print(f"âš ï¸ Lá»—i kiá»ƒm tra ngÃ y Ä‘Äƒng: {e}")
    return None


# ğŸ”¹ HÃ m láº¥y danh sÃ¡ch sáº£n pháº©m tá»« trang category

# ğŸ”¹ HÃ m crawl dá»¯ liá»‡u sáº£n pháº©m tá»« danh sÃ¡ch link

# ğŸ”¹ HÃ m lÆ°u dá»¯ liá»‡u vÃ o Google Sheets
def save_to_google_sheets(data):
    try:
        scope = ["https://spreadsheets.google.com/feeds", "https://www.googleapis.com/auth/drive"]
        creds = ServiceAccountCredentials.from_json_keyfile_name(r"C:\ScrawlData\CrawlNewData\credentials.json", scope)
        client = gspread.authorize(creds)
        sheet_id = "1c_omcHE3AMkUBxFS1Cckwcw0A-fBX5FV6dbJYn_Jg5s"
        sheet = client.open_by_key(sheet_id).worksheet("Láº¥y Link")

        sheet.clear()
        sheet.append_row(["Link Trang", "Link Sáº£n Pháº©m", "GiÃ¡ Trá»‹ Tá»« Torunstyle","NgÃ y ÄÄƒng"])

        if data:
            sheet.update(f"A2:D{len(data)+1}", data)  # Ghi dá»¯ liá»‡u hÃ ng loáº¡t

        print("âœ… Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c lÆ°u vÃ o Google Sheets.")
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u dá»¯ liá»‡u vÃ o Google Sheets: {e}")

def add_link(scraped_data ,driver,url,type):
    driver.get(url)
    count = 0
    product_links = []
    while True:
        try:
            if(count == 1): break
            count =count +1 
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.TAG_NAME, 'body'))
            )
            links = driver.find_elements(By.CSS_SELECTOR, "a[aria-label]")
            for link in links :
                if link.get_attribute("aria-label") != "Menu": 
                    href = link.get_attribute("href")
                    if href and href not in product_links and  "https" in href and "instagram" not in href and "facebook" not in href and "twitter" not in href:
                        # if check_publish_date(browser, href):
                        product_links.append(href)
            try:            
                next_button = WebDriverWait(driver, 5).until(
                    EC.element_to_be_clickable((By.CSS_SELECTOR, "a.next.page-number"))
                )
                aria_disabled = next_button.get_attribute("aria-disabled")

                if aria_disabled == "true":
                    break  # KhÃ´ng cÃ²n trang tiáº¿p theo

                next_button.click()
            # Äá»£i trang má»›i táº£i hoÃ n táº¥t
            # WebDriverWait(browser, 10).until(
            #     EC.staleness_of(images[0])  # Chá» cho trang cÅ© biáº¿n máº¥t
            # )
            except (NoSuchElementException, TimeoutException):
                break
        except TimeoutException:
            print("KhÃ´ng tÃ¬m tháº¥y pháº§n tá»­ mong muá»‘n, thoÃ¡t chÆ°Æ¡ng trÃ¬nh.")
            break
    print('product_links',product_links)
    scrawlNew = False
    for index, link in enumerate(product_links):
        try:
            driver.get(link)
            date = check_publish_date(driver,link)
            if date :
                if scrawlNew == False :
                    scraped_data.append([url,link,type,date.strftime('%Y-%m-%d %H:%M:%S')])
                    scrawlNew = True
                else:
                    scraped_data.append(["",link,type,date.strftime('%Y-%m-%d %H:%M:%S')])
                
        except Exception as e:
            print(f"âŒ Lá»—i khi crawl {link}: {e}")
            driver.quit()
            time.sleep(5)
            driver = create_driver()


# ğŸ”¹ Cháº¡y chÆ°Æ¡ng trÃ¬nh chÃ­nh
if __name__ == "__main__":
    driver = create_driver()
    url_data = get_urls_from_sheets()
    print('url_data',url_data)
    scraped_data = []
    for item in url_data:
        print('url,type', item['url'],item['types'])
        add_link(scraped_data,driver, item['url'],item['types'])
        
    print('scraped_data',scraped_data)
    
    

    save_to_google_sheets(scraped_data)

    driver.quit()
    print("ğŸ‰ HoÃ n táº¥t quÃ¡ trÃ¬nh crawl dá»¯ liá»‡u!")
