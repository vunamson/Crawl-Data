import os
import asyncio
import aiohttp
import pandas as pd
from openpyxl import Workbook
from bs4 import BeautifulSoup
from slugify import slugify
import json
from datetime import datetime


# Ghi d·ªØ li·ªáu v√†o file Excel
def save_to_excel(data, file_name):
    wb = Workbook()
    ws = wb.active
    ws.append(["store", "title", "product_link", "image_link", "date", "sku"])
    
    for row in data:
        ws.append(list(row.values()))
    
    wb.save(file_name)

# Ki·ªÉm tra xem s·∫£n ph·∫©m c√≥ trong gi·ªè h√†ng kh√¥ng
def is_product_in_cart(soup):
    try:
        cart_script_tag = soup.find('script', string=lambda t: t and 'wpmDataLayer' in t)
        if not cart_script_tag:
            return False

        cart_data_str = cart_script_tag.string.split('window.wpmDataLayer = ')[-1].strip().rstrip(';')
        cart_data = json.loads(cart_data_str)
        cart_items = cart_data.get("cart", {}).get("items", [])
        return len(cart_items) > 0
    except Exception as e:
        print(f"L·ªói khi ki·ªÉm tra gi·ªè h√†ng: {e}")
        return False

# Ph√¢n t√≠ch HTML ƒë·ªÉ l·∫•y th√¥ng tin s·∫£n ph·∫©m
def parse_html(html):
    soup = BeautifulSoup(html, 'html.parser')
    title = soup.find('h1')
    title_text = title.text.strip() if title else 'No Title'

    script_tag = soup.find('script', type="application/ld+json")
    sku, date_published = "Not Found", "Not Found"
    image_links = []

    if script_tag:
        try:
            data = json.loads(script_tag.string)
            if isinstance(data, dict) and data.get("@type") == "Product":
                # L·∫•y t√™n s·∫£n ph·∫©m t·ª´ d·ªØ li·ªáu JSON
                title_text = data.get("name", title_text)
                # L·∫•y sku v√† chuy·ªÉn ƒë·ªïi date
                text = data.get("sku", "Not Found")
                sku = text.split("_")[0]
                date_string =  str(text.split("_")[1])[:8]
                print('date_string' ,date_string)
                if date_string:
                    try:
                        date_obj = datetime.strptime(date_string, '%Y%m%d')
                        date_published = date_obj.strftime('%Y/%m/%d')
                    except ValueError:
                        date_published = "Invalid Date"

                # L·∫•y t·∫•t c·∫£ c√°c ·∫£nh
                img_tags = soup.find_all('img', class_="sgub-product-image-main")
                image_links = img_tags[0].get('data-src') if img_tags else ""
        except json.JSONDecodeError:
            print("L·ªói gi·∫£i m√£ JSON trong th·∫ª script")
    product_link = soup.find("link", rel="canonical")["href"] if soup.find("link", rel="canonical") else None
    if product_link : 
        return {
            'title': title_text,
            'product_link': product_link,
            'image_link': image_links if image_links else '',
            'date_published': date_published,
            'sku': sku,
        }

# H√†m crawl m·ªôt trang c·ª• th·ªÉ v·ªõi gi·ªõi h·∫°n request
async def crawl_page(sem, session, object_id, max_retries=1):
    url = f"https://lipomarts.com/?attachment_id={object_id}"

    async with sem:  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi
        for attempt in range(max_retries):
            try:
                async with session.get(url, timeout=10) as response:
                    if response.status == 524:
                        print(f"‚ö†Ô∏è L·ªói 524 Timeout - Th·ª≠ l·∫°i {attempt + 1}/{max_retries} cho {object_id}")
                        await asyncio.sleep(5)
                        continue

                    if response.status != 200:
                        print(f"‚ùå L·ªói HTTP {response.status} - B·ªè qua {object_id}")
                        return None

                    html = await response.text()

                    # N·∫øu trang l·ªói 524 xu·∫•t hi·ªán trong n·ªôi dung, b·ªè qua request
                    if "Error code 524" in html:
                        print(f"‚ö†Ô∏è Trang {object_id} b·ªã l·ªói 524 - B·ªè qua.")
                        return None

                    data = parse_html(html)
                    slug = slugify(data['title'], allow_unicode=True).lower()
                    if data : 
                        result = {
                            'store': 'lipomarts.com',
                            'title': data['title'],
                            'product_link': data['product_link'],
                            'image_link': data['image_link'],
                            'date_published': data['date_published'],
                            'sku': data['sku'],
                        }

                    print(f"‚úÖ {object_id}: {data['title']}")
                    return result
            except asyncio.TimeoutError:
                print(f"‚ö†Ô∏è Request Timeout {object_id} - Th·ª≠ l·∫°i {attempt + 1}/{max_retries}")
                await asyncio.sleep(5)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi crawl {object_id} {url}: {e}")
                return None

    print(f"‚ùå B·ªè qua {object_id} sau {max_retries} l·∫ßn th·ª≠ th·∫•t b·∫°i")
    return None


# H√†m ch√≠nh ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh v·ªõi gi·ªõi h·∫°n 30 request ƒë·ªìng th·ªùi
async def main():
    start, end = 1210000, 1220000
    object_ids = list(range(start, end + 1))

    # Gi·ªõi h·∫°n s·ªë lu·ªìng request (c√≥ th·ªÉ thay ƒë·ªïi gi√° tr·ªã n√†y ƒë·ªÉ ƒëi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi)
    num_of_concurrent_requests = 5  # ƒêi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi t·∫°i ƒë√¢y
    sem = asyncio.Semaphore(num_of_concurrent_requests)

    async with aiohttp.ClientSession() as session:
        tasks = [asyncio.create_task(crawl_page(sem, session, object_id)) for object_id in object_ids]
        results = await asyncio.gather(*tasks, return_exceptions=True)

    valid_results = [r for r in results if r]
    save_to_excel(valid_results, "crawled_lipomarts_datav2-28-03.xlsx")

    print("üéâ Crawl ho√†n t·∫•t!")
    print("üìä T·ªïng s·ªë s·∫£n ph·∫©m crawl ƒë∆∞·ª£c:", len([r for r in results if r]))

if __name__ == "__main__":
    asyncio.run(main())
