import os
import asyncio
import random
import aiohttp
import pandas as pd
from openpyxl import Workbook
from bs4 import BeautifulSoup
from slugify import slugify
import csv
import json
import time

# Ghi d·ªØ li·ªáu v√†o file Excel
HEADERS = [
    "ID","Type","SKU","Name","Published","Is featured?","Visibility in catalog","Short description","Description",
    "Date sale price starts","Date sale price ends","Tax status","Tax class","In stock?","Stock","Low stock amount",
    "Backorders allowed?","Sold individually?","Weight (kg)","Length (cm)","Width (cm)","Height (cm)","Allow customer reviews?",
    "Purchase note","Sale price","Regular price","Categories","Tags","Shipping class","Images",
    "Download limit","Download expiry days","Parent","Grouped products","Upsells","Cross-sells",
    "External URL","Button text","Position",
    "Attribute 1 name","Attribute 1 value(s)","Attribute 1 visible","Attribute 1 global","Attribute 1 default",
    "Attribute 2 name","Attribute 2 value(s)","Attribute 2 visible","Attribute 2 global","Attribute 2 default",
    "Attribute 3 name","Attribute 3 value(s)","Attribute 3 visible","Attribute 3 global","Attribute 3 default",
    "Meta: hwp_product_gtin","ID site"
]

def save_to_csv(data, file_name):
    """Ghi d·ªØ li·ªáu ra file CSV."""
    with open(file_name, mode='w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(HEADERS)
        writer.writerows(data)
    print(f"‚úÖ ƒê√£ l∆∞u {len(data)} d√≤ng v√†o {file_name}")

# 1. Chu·∫©n b·ªã danh s√°ch User-Agent th·∫≠t
USER_AGENTS = [
    # M·ªôt v√†i User-Agent th√¥ng d·ª•ng c·ªßa Chrome, Firefox...
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 "
    "(KHTML, like Gecko) Version/14.0 Safari/605.1.15",
    # b·∫°n c√≥ th·ªÉ th√™m list t·ª´ fake-useragent
]

# 2. T·∫°o session v·ªõi headers m·∫∑c ƒë·ªãnh
async def create_session():
    headers = {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
        "Accept-Language": "en-US,en;q=0.9",
        "Referer": "https://serenashirt.com/",
        "Connection": "keep-alive",
    }
    # trust_env=True ƒë·ªÉ d√πng proxy/setting m√¥i tr∆∞·ªùng n·∫øu c√≥
    return aiohttp.ClientSession(headers=headers, trust_env=True)

# 3. Tr∆∞·ªõc khi crawl, gh√© qua trang ch·ªß ƒë·ªÉ l·∫•y cookie phi√™n
async def init_cookies(session):
    try:
        async with session.get("https://serenashirt.com/", timeout=10) as resp:
            # ƒë·ªçc v√† b·ªè qua n·ªôi dung, cookie s·∫Ω ƒë∆∞·ª£c l∆∞u t·ª± ƒë·ªông trong session
            await resp.text()
    except Exception as e:
        print("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c cookie t·ª´ trang ch·ªß:", e)

# Ki·ªÉm tra xem s·∫£n ph·∫©m c√≥ trong gi·ªè h√†ng kh√¥ng

# Ph√¢n t√≠ch HTML ƒë·ªÉ l·∫•y th√¥ng tin s·∫£n ph·∫©m
def parse_html(html):
    soup = BeautifulSoup(html, 'html.parser')
    # 1. L·∫•y name
    h1 = soup.find('h1', class_='product-title product_title entry-title')
    name = h1.get_text(strip=True) if h1 else 'No Title'

    # 2. L·∫•y list image links ch·ªâ khi class list == ['attachment-woocommerce_thumbnail']
    img_tags = soup.find_all('img', class_='attachment-woocommerce_thumbnail')
    img_links = []
    for img in img_tags:
        classes = img.get('class', [])
        if classes == ['attachment-woocommerce_thumbnail']:
            src = img.get('src')
            if src:
                img_links.append(src)
    img_links_str = ', '.join(img_links)

    return {
        'name': name,
        'img_links_str': img_links_str
    }


# H√†m crawl m·ªôt trang c·ª• th·ªÉ v·ªõi gi·ªõi h·∫°n request
async def crawl_page(sem, session, object_id, max_retries=3):
    url = f"https://serenashirt.com/?post_type=prudut&page_id={object_id}"

    async with sem:  # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng request ƒë·ªìng th·ªùi
        for attempt in range(max_retries):
            try:
                async with session.get(url, timeout=10) as response:
                    if response.status == 524:
                        print(f"‚ö†Ô∏è L·ªói 524 Timeout - Th·ª≠ l·∫°i {attempt + 1}/{max_retries} cho {object_id}")
                        # await asyncio.sleep(5)
                        continue
                    if response.status == 500:
                        time.sleep(60)
                        print(f"‚ùå limit request")
                        return None

                    if response.status != 200:
                        print(f"‚ùå L·ªói HTTP {response.status} - B·ªè qua {object_id}")
                        return None

                    html = await response.text()

                    # N·∫øu trang l·ªói 524 xu·∫•t hi·ªán trong n·ªôi dung, b·ªè qua request
                    if "Error code 524" in html:
                        print(f"‚ö†Ô∏è Trang {object_id} b·ªã l·ªói 524 - B·ªè qua.")
                        return None

                    data = parse_html(html)
                    name = data['name']
                    img_links_str = data['img_links_str']

                    sku_prefix = ""  # v√≠ d·ª•
                    record_idx = object_id - 0  # tu·ª≥ b·∫°n t√≠nh
                    description = ""         # tu·ª≥ logic
                    price = ""  

                    row = [
                        "",                                 # ID
                        "simple",                           # Type
                        f"{100000 + 2 * record_idx}",  # SKU
                        name,                               # Name
                        1, 0, "visible", "",                # Published, Is featured?, Visibility, Short desc
                        description,                        # Description
                        "", "",                            # Date sale price starts/ends
                        "taxable", "",                      # Tax status/Class
                        1, "", "", 0, 0,                   # In stock?, Stock, Low stock amt, Backorders, Sold individually
                        "", "", "", "",                    # Weight, Length, Width, Height
                        1, "", "",                         # Allow reviews?, Purchase note, Sale price placeholder
                        price,                             # Regular price (n·∫øu b·∫°n ƒë·ªÉ ng∆∞·ª£c th√¨ ho√°n v·ªã)
                        sku_prefix, sku_prefix, "",        # Categories, Tags, Shipping class
                        img_links_str,
                        "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "", "",                     # Images
                        object_id,
                        # ph·∫ßn c√≤n l·∫°i l√† c√°c c·ªôt r·ªóng ƒë·∫øn h·∫øt
                    ]
                    print(f"‚úÖ {object_id}: {data['name']}")
                    return row
            except asyncio.TimeoutError:
                print(f"‚ö†Ô∏è Request Timeout {object_id} - Th·ª≠ l·∫°i {attempt + 1}/{max_retries}")
                # await asyncio.sleep(5)
            except Exception as e:
                print(f"‚ö†Ô∏è L·ªói khi crawl {object_id} {url}: {e}")
                return None

    print(f"‚ùå B·ªè qua {object_id} sau {max_retries} l·∫ßn th·ª≠ th·∫•t b·∫°i")
    return None

# H√†m ch√≠nh ƒë·ªÉ ch·∫°y ch∆∞∆°ng tr√¨nh v·ªõi gi·ªõi h·∫°n 30 request ƒë·ªìng th·ªùi
async def main():
    start_id = 1562999
    end_id   = 1592999

    sem = asyncio.Semaphore(5)  # Gi·ªõi h·∫°n 30 request ƒë·ªìng th·ªùi
      # 1. Kh·ªüi t·∫°o session v·ªõi headers gi·∫£ l·∫≠p tr√¨nh duy·ªát
    session = await create_session()

    # 2. Gh√© qua trang ch·ªß ƒë·ªÉ thu cookie phi√™n
    await init_cookies(session)
    tasks = [
        crawl_page(sem, session, pid, max_retries=1)
        for pid in range(start_id, end_id + 1)
    ]
    results = await asyncio.gather(*tasks)
    # async with aiohttp.ClientSession() as session:
    #     tasks = [
    #         crawl_page(sem, session, pid,1) 
    #         for pid in range(start_id, end_id + 1)
    #     ]
    #     results = await asyncio.gather(*tasks)

    rows = [r for r in results if r]
    save_to_csv(rows, "crawled_serenashirt_data_1562999-1592999.csv")

    print("üéâ Crawl ho√†n t·∫•t!")
    print("üìä T·ªïng s·ªë s·∫£n ph·∫©m crawl ƒë∆∞·ª£c:", len([r for r in results if r]))

if __name__ == "__main__":
    asyncio.run(main())